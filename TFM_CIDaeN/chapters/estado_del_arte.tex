\chapter{Estado del arte}\label{estado_del_arte}

\section{Deep Learning}

El campo del reconocimiento de acciones se ha vuelto un área de investigación cada vez más activa en los últimos años (ver \href{https://paperswithcode.com/task/action-recognition-in-videos}{\textit{Action Recognition}}). Si bien las acciones humanas se pueden reconocer de diferentes formas, como el \textit{optical flow} o en \textit{representaciones de esqueletos}, en este trabajo nos centramos en la clasificación de videos. La clasificación de vídeos consiste en la obtención de una etiqueta representativa del contenido del vídeo, dados los frames que lo componen (ver referencia \href{https://paperswithcode.com/task/video-classification}{\textit{Video Classification}}).

Aunque los primeros datasets de actividades en humanos empiezan a aparecer a princi-pios de la década de 2010 con datasets como HMDB51 o UCF101, no es hasta la introducción del dataset \textit{Kinetics 400} (ver \cite{Kinetics400}) que se se plantea el primer punto de referencia para la clasificación de acciones en humanos. Debido al tamaño del conjunto de datos, se hace posible entrenar redes neuronales desde cero y permite probar distintas arquitecturas sobre el mismo que posteriormente se pueden ajustar a tareas más a la medida por medio de \textit{fine-tuning} (ver \cite{I3D}).

En el momento de plantearse este trabajo, uno de los modelos más avanzados en el campo del reconocimiento de acciones es \textit{MoViNets}, una familia de modelos eficientes tanto en el uso de memoria como en computación, que permite operar con videos en \textit{streaming}, logrando  \textit{accuracies} más elevados en comparación con otros modelos desarrollados en la literatura, en sus arquitecturas más complejas (ver \cite{MoViNets})\footnote{Actualmente el panorama de modelos entrenados sobre Kinetics 600 ha evolucionado mucho, como puede verse en \href{https://paperswithcode.com/sota/action-classification-on-kinetics-600}{Action Classification on Kinetics-600}, incluso añadiendo componentes de audio y texto (subtítulos): \cite{Merlot}.}, que ofrecen resultados muy prometedores sobre \textit{Kinetics 600}, una mejora sobre su antecesor \textit{Kinetics 400}: \cite{Kinetics600}, y hasta la fecha el dataset de referencia para este tipo de modelos.

Esta familia de modelos se compone de dos versiones diferentes, que en el paper original llaman \textit{base} y \textit{stream}, con distintos "niveles" de complejidad (desde a0 hasta a5, donde a0 es una arquitectura simplificada que parte de MobileNetV3, ver \cite{MobileNetV3}), y a5-base obtiene resultados al nivel de X3D \cite{X3D} (ver Tabla 17 en y Tabla 22 respectivamente en \cite{MoViNets}) con un menor número de recursos computa-cionales. La gran novedad que aporta este trabajo es la variante \textit{stream}, que permite tratar videos de gran longitud sin tener que procesar todos los frames por completo antes de devolver una predicción, con lo que se hace posible la inferencia en videos de gran longitud y con dispositivos de menor capacidad (ver \href{https://blog.tensorflow.org/2022/04/video-classification-on-edge-devices.html}{aquí}).

Los autores de \textit{MoViNets} ofrecen el siguiente \href{https://colab.research.google.com/github/tensorflow/models/blob/master/official/projects/movinet/movinet_tutorial.ipynb}{\textit{colab notebook}} en el que muestran ejem-plos de como hacer inferencia tanto con los modelos \textit{base} como \textit{stream}, exportar el modelo a \textit{Tensorflow Lite} para hacer predicción en dispositivos móviles, y como hacer \textit{fine-tuning}, utilizando para ello el modelo \textit{MoViNet-A0-Base}. Como se puede ver en el mismo ejemplo, tras solo 3 \textit{epochs}, haciendo fine tuning sobre el dataset \href{https://www.tensorflow.org/datasets/catalog/ucf101}{UCF101} consiguen un top 1 accuracy de $0.9329$ y $0.8514$ en training y validación respectivamente, por lo que parece un buen punto de partida para aplicar al conjunto de datos creado en este trabajo.

En cuanto a las arquitecturas de \textit{MoViNets stream}, si bien a la hora de hacer inferencia todo funciona correctamente, y el procedimiento para hacer fine-tuning debería ser análogo, no he visto posibilidad de reproducir el entrenamiento. En el siguiente \href{https://github.com/tensorflow/models/issues/10730}{issue 10730} se puede ver los errores. Este comportamiento parece relacionado con los siguientes issues \href{https://github.com/tensorflow/models/issues/10463#issuecomment-1019395406_}{10463} y \href{https://github.com/tensorflow/models/issues/10515}{10515}, y actualmente se encuentran o bien abiertos esperando respuesta, o se han cerrado sin respuesta. Debido a esto, se ha optado por centrarse en la arquitectura de los modelos \textit{base}.


\section{Cloud}

A la hora de desplegar una aplicación que hace uso de machine learning (ML), los provee-dores cloud ofrecen servicios que automatizan una gran parte del ciclo de vida del modelo, tanto en AWS (con \href{https://aws.amazon.com/sagemaker/?nc1=h_ls}{\textit{Amazon SageMaker}}), Azure: (\href{https://azure.microsoft.com/es-es/services/machine-learning/#product-overview}{\textit{Azure ML}}) o Google (con \href{https://cloud.google.com/datalab/docs}{\textit{Google Datalab}}) por ejemplo.

En este trabajo se ha optado por desplegar el modelo en AWS, partiendo de cero, compo-niendo los servicios base que ofrece AWS para servir la aplicación por medio de \href{https://aws.amazon.com/ec2/?nc1=h_ls}{EC2}, y obtener las predicciones del modelo haciendo llamadas a una función \href{https://aws.amazon.com/lambda/?nc1=h_ls}{función lambda} respal-dada por un contenedor de docker en \href{https://aws.amazon.com/ecr/?nc1=h_ls}{ECR}.

\section{Trabajos relacionados}

El presente trabajo busca un punto de partida de cara a distinguir movimientos y contar repeticiones en un evento de CrossFit. Algunos trabajos relacionado con esta temática se pueden encontrar por ejemplo en 
\cite{FitnessMovementTypes}, donde hacen uso de \textit{Yolo 4} (ver el artículo \cite{Yolo4}) y \href{https://mediapipe.dev/}{\textit{Mediapipe}} para detectar y clasificar distintos movimien-tos de fitness, o en \cite{ClassifyFunctionalFitness}, donde se hace uso de sensores portátiles para clasificar movimientos a lo largo de un \textit{workout}. 

Fuera de la literatura, un artículo hace uso de \textit{MoViNets}.
En concreto: \href{https://blog.ml6.eu/sports-video-analysis-in-the-real-world-realtime-tennis-action-recognition-using-movinet-stream-813200aa589f}{\textit{tennis-action-recognition}} hace uso de \textit{MoViNet stream} para clasificar movimientos en un evento de tenis\footnote{Si bien el trabajo presentado en este blog encaja en gran medida con lo que se busca con este trabajo, no ha sido replicar los resultados, ni ha sido posible encontrar referencias del autor.}.

En este otro \href{https://towardsdatascience.com/how-i-created-the-workout-movement-counting-app-using-deep-learning-and-optical-flow-89f9d2e087ac}{artículo de \textit{towardsdatascience}} el autor hace uso de un modelo de clasifica-ción sobre la representación \href{https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html}{\textit{Optical Flow}} utilizando una red neuronal propia, para contar repeticiones de movimientos que parece ofrecer buenos resultados.

Finalmente se ha decidido hacer uso de una versión de \textit{MoViNets base} como modelo para la clasificación de movimientos, ya que ofrece un buen punto de partida para clasificar acciones (en este caso ejercicios de funcionales o de CrossFit), y debido a que el modelo tiene un tamaño adecuado para ser desplegado sin hacer un uso intensivo de recursos (permite hacer inferencia sin necesidad de utilizar GPU para los casos estudiados), lo que permite hacer uso del mismo por medio de una aplicación por un usuario sin conocimiento técnico.

